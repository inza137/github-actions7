# .github/workflows/containerized_data_job.yml

# Name of your workflow, displayed on the Actions tab
name: Containerized Data Processing Job

# How the workflow is triggered. 'workflow_dispatch' allows manual triggering.
on:
  workflow_dispatch:
  push: # You can also add other triggers like push
    branches:
      - main

# Defines the jobs that run as part of this workflow
jobs:
  # Name of the job
  process_data_in_container:
    name: "Run Data Processing in Python Container"
    # The runner environment that will host the Docker container
    runs-on: ubuntu-latest

    # This 'container' block specifies that the entire job (all steps below)
    # will run inside this Docker container, unless a step specifies its own container.
    container:
      # The Docker image to use. This pulls 'python' version '3.9-slim' from Docker Hub.
      # 'slim' images are generally smaller.
      image: python:3.9-slim
      # Optional: You can pass options to the 'docker run' command.
      # For example, if you needed to mount specific volumes or set environment variables for the container.
      # options: --cpus 1 --memory 1g
      # Optional: Define environment variables specifically for this container.
      # env:
      #   MY_CONTAINER_VAR: "Hello from container env"

    # Steps to be executed within the job (and thus, within the container)
    steps:
      # Step 1: Checkout your repository code
      # The GITHUB_WORKSPACE (where your repo is checked out) is automatically
      # mounted into the container, typically at /github/workspace.
      - name: "Checkout repository code"
        uses: actions/checkout@v4
        # 'actions/checkout' will place your repository files into the GITHUB_WORKSPACE,
        # making them accessible inside the container.

      - name: "Display initial workspace contents from container"
        # This 'run' command executes inside the 'python:3.9-slim' container.
        run: |
          echo "Running inside container: $(cat /etc/os-release | grep PRETTY_NAME || echo 'OS info not found')"
          echo "Python version in container:"
          python --version
          echo "Pip version in container:"
          pip --version
          echo "Current directory (should be GITHUB_WORKSPACE): $(pwd)"
          echo "Listing GITHUB_WORKSPACE contents:"
          ls -la $GITHUB_WORKSPACE

      - name: "Install Python dependencies (pandas) in container"
        # This command runs inside the container.
        # The installed packages will be available to subsequent Python scripts in this job.
        run: |
          echo "Installing pandas library using pip..."
          pip install pandas --quiet --no-cache-dir 
          # --quiet reduces log verbosity, --no-cache-dir saves space if you don't need pip's own cache layer here
          echo "Pandas installed."

      - name: "Run the Python data processing script"
        # This executes your Python script using the Python interpreter inside the container.
        # The script is accessed from the checked-out GITHUB_WORKSPACE.
        # Adjust the path if you placed your script elsewhere.
        run: python src/process_inventory_data.py

      # Optional Step 5: Upload the processed data as an artifact
      # This shows that files created by the script inside the container (in GITHUB_WORKSPACE)
      # are accessible by the host runner for actions like uploading artifacts.
      - name: "Upload processed data artifact"
        uses: actions/upload-artifact@v4
        with:
          name: processed-inventory-data # Name of the artifact bundle
          path: ${{ github.workspace }}/processed_data_output/transformed_inventory.csv # Path to the file generated by the script
          # The path is relative to GITHUB_WORKSPACE on the runner, which maps to the container's workspace.
          if-no-files-found: error # Fail if the output file isn't found